
---
title: "Bug #400: Fwd: [RFC PATCH] tcp: limit data skbs in qdisc layer"
subject: "Fwd: [RFC PATCH] tcp: limit data skbs in qdisc layer"
date: 2012-07-05T09:12:55
updated: 2013-04-11T04:26:21
type: issue
author: David Taht
id: 400
issue_status: Closed
priority: Normal
assignee: Dave TÃ¤ht
aliases:
    - /issues/400
---

{{< issue_description >}}
---------- Forwarded message ----------\
From: Eric Dumazet <eric.dumazet@gmail.com>\
Date: Wed, Jul 4, 2012 at 6:11 AM\
Subject: \[RFC PATCH\] tcp: limit data skbs in qdisc layer\
To: David Miller <davem@davemloft.net>\
Cc: Yuchung Cheng <ycheng@google.com>, Dave Taht\
<dave.taht@gmail.com>, netdev <netdev@vger.kernel.org>,\
codel@lists.bufferbloat.net, Tom Herbert <therbert@google.com>, Matt\
Mathis <mattmathis@google.com>, Nandita Dukkipati\
<nanditad@google.com>, Neal Cardwell <ncardwell@google.com>, Andrew\
McGregor <andrewmcgr@gmail.com>

On Fri, 2012-06-29 at 06:51 +0200, Eric Dumazet wrote:

&gt; My long term plan is to reduce number of skbs queued in Qdisc for
TCP\
&gt; stack, to reduce RTT (removing the artificial RTT bias because of
local\
&gt; queues)

preliminar patch to give the rough idea :

sk-&gt;sk\_wmem\_alloc not allowed to grow above a given limit,\
allowing no more than \~4 segments \[1\] per tcp socket in qdisc layer
at a\
given time. (if TSO is enabled, then a single TSO packet hits the limit)

This means we divert sock\_wfree() to a tcp\_wfree() handler, to\
queue/send following frames when skb\_orphan() is called for the
already\
queued skbs.

Note : While this lowers artificial cwnd and rtt, this means more work\
has to be done by tx completion handler (softirq instead of preemptable\
process context)

To reduce this possible source of latency, we can skb\_try\_orphan(skb)
in\
NIC drivers ndo\_start\_xmit() instead of waiting TX completion, but do\
this orphaning only on BQL enabled drivers, or in drivers known to\
possibly delay TX completion (NIU is an example, as David had to revert\
BQL because of this delaying)

results on my dev machine (tg3 nic) are really impressive, using\
standard pfifo\_fast, and with or without TSO/GSO

I no longer have 3MBytes backlogged in qdisc by a single netperf\
session, and both side socket autotuning no longer use 4 Mbytes.

tcp\_write\_xmit() call is probably very naive and lacks proper tests.

Note :

\[1\] because sk\_wmem\_alloc accounts skb truesize, mss\*4 test allow
less\
then 4 frames, but it seems ok.

drivers/net/ethernet/broadcom/tg3.c | 1\
include/linux/skbuff.h | 6 **+\
include/net/sock.h | 1\
net/ipv4/tcp\_output.c | 44 **+-\
4 files changed, 51 insertions(+), 1 deletion(-)

diff --git a/drivers/net/ethernet/broadcom/tg3.c\
b/drivers/net/ethernet/broadcom/tg3.c\
index e47ff8b..ce0ca96 100644\
--- a/drivers/net/ethernet/broadcom/tg3.c\
**+ b/drivers/net/ethernet/broadcom/tg3.c\
`@ -7020,6 +7020,7 `@ static netdev\_tx\_t tg3\_start\_xmit(struct
sk\_buff\
\*skb, struct net\_device \*dev)

skb\_tx\_timestamp(skb);\
netdev\_tx\_sent\_queue(txq, skb-&gt;len);\
+ skb\_try\_orphan(skb);

/\* Sync BD data before updating mailbox \*/\
wmb();\
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h\
index 885c9bd..c4d4d15 100644\
--- a/include/linux/skbuff.h\
**+ b/include/linux/skbuff.h\
`@ -1667,6 +1667,12 `@ static inline void skb\_orphan(struct sk\_buff
\*skb)\
skb-&gt;sk = NULL;\
}

+static inline void skb\_try\_orphan(struct sk\_buff \*skb)\
+{\
+ if (!skb\_shinfo(skb)-&gt;tx\_flags)\
+ skb\_orphan(skb);\
+}\
+\
/\*\*\
\* \_\_skb\_queue\_purge - empty a list\
\* `list: list to empty
diff --git a/include/net/sock.h b/include/net/sock.h
index 640432a..2ce17b1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
`@ -1389,6 +1389,7 @@ extern void release\_sock(struct sock \*sk);

/\* BH context may only use the following locking interface. \*/\
\#define bh\_lock\_sock(\_*sk)
spin\_lock(&((*\_sk)-&gt;sk\_lock.slock))\
+\#define bh\_trylock\_sock(\_*sk)
spin\_trylock(&((*\_sk)-&gt;sk\_lock.slock))\
\#define bh\_lock\_sock\_nested(\_\_sk) \\\
spin\_lock\_nested(&((\_\_sk)-&gt;sk\_lock.slock), \\\
SINGLE\_DEPTH\_NESTING)\
diff --git a/net/ipv4/tcp\_output.c b/net/ipv4/tcp\_output.c\
index c465d3e..4e6ef82 100644\
--- a/net/ipv4/tcp\_output.c\
**+ b/net/ipv4/tcp\_output.c\
`@ -65,6 +65,8 `@ int sysctl\_tcp\_slow\_start\_after\_idle
\_\_read\_mostly = 1;\
int sysctl\_tcp\_cookie\_size \_\_read\_mostly = 0; /\* TCP\_COOKIE\_MAX
\*/\
EXPORT\_SYMBOL\_GPL(sysctl\_tcp\_cookie\_size);

+static bool tcp\_write\_xmit(struct sock \*sk, unsigned int mss\_now,
int nonagle,\
+ int push\_one, gfp\_t gfp);

/\* Account for new data that has been sent to the network. \*/\
static void tcp\_event\_new\_data\_sent(struct sock \*sk, const struct
sk\_buff \*skb)\
`@ -783,6 +785,34 `@ static unsigned int\
tcp\_established\_options(struct sock \*sk, struct sk\_buff \*skb\
return size;\
}

+/\*\
+ \* Write buffer destructor automatically called from kfree\_skb.\
+ \*/\
+void tcp\_wfree(struct sk\_buff \*skb)\
+{\
+ struct sock \*sk = skb-&gt;sk;\
+ unsigned int len = skb-&gt;truesize;\
+\
+ atomic\_sub(len - 1, &sk-&gt;sk\_wmem\_alloc);\
+ if (bh\_trylock\_sock(sk)) {\
+ if (!sock\_owned\_by\_user(sk)) {\
+ if ((1 &lt;&lt; sk-&gt;sk\_state) &\
+ (TCPF\_CLOSE\_WAIT | TCPF\_ESTABLISHED))\
+ tcp\_write\_xmit(sk,\
+ tcp\_current\_mss(sk),\
+ 0, 0,\
+ GFP\_ATOMIC);\
+ } else {\
+ /\* TODO : might signal something here\
+ \* so that user thread can call tcp\_write\_xmit()\
+ \*/\
+ }\
+ bh\_unlock\_sock(sk);\
+ }\
+\
+ sk\_free(sk);\
+}\
+\
/\* This routine actually transmits TCP packets queued in by\
\* tcp\_do\_sendmsg(). This is used by both the initial\
\* transmission and possible later retransmissions.\
`@ -844,7 +874,11 `@ static int tcp\_transmit\_skb(struct sock \*sk,\
struct sk\_buff \*skb, int clone\_it,

skb\_push(skb, tcp\_header\_size);\
skb\_reset\_transport\_header(skb);\
- skb\_set\_owner\_w(skb, sk);\
+\
+ skb\_orphan(skb);\
+ skb-&gt;sk = sk;\
+ skb-&gt;destructor = tcp\_wfree;\
+ atomic\_add(skb-&gt;truesize, &sk-&gt;sk\_wmem\_alloc);

/\* Build TCP header and checksum it. \*/\
th = tcp\_hdr(skb);\
`@ -1780,6 +1814,14 `@ static bool tcp\_write\_xmit(struct sock \*sk,\
unsigned int mss\_now, int nonagle,\
while ((skb = tcp\_send\_head(sk))) {\
unsigned int limit;

+ /\* yes, sk\_wmem\_alloc accounts skb truesize, so mss\_now \* 4\
+ \* is a lazy approximation of our needs, but it seems ok\
+ \*/\
+ if (atomic\_read(&sk-&gt;sk\_wmem\_alloc) &gt;= mss\_now \* 4) {\
+ pr\_debug("here we stop sending frame because\
sk\_wmem\_alloc %d\\n",\
+ atomic\_read(&sk-&gt;sk\_wmem\_alloc));\
+ break;\
+ }\
tso\_segs = tcp\_init\_tso\_segs(sk, skb, mss\_now);\
BUG\_ON(!tso\_segs);


{{< /issue_description >}}

## History
{{< issue_journal date="2012-10-26T07:41:47" author="David Taht" >}}
TCP small queuses stuff is upstream in 3.6 for this now
{{< /issue_journal >}}
{{< issue_journal date="2013-04-11T04:26:21" author="David Taht" >}}

{{< /issue_journal >}}

