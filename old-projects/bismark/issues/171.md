
---
title: "Bug #171: Set default QoS to something reasonable"
subject: "Set default QoS to something reasonable"
date: 2011-05-19T17:52:41
updated: 2011-05-27T19:25:41
type: issue
author: Nick Feamster
id: 171
issue_status: Closed
priority: Normal
assignee: Nick Feamster
aliases:
    - /issues/171
---

{{< issue_description >}}
People are already complaining about the default QoS setting.

We should make the default work. Let's at least set it to something
reasonable. I just ran a speedtest from here. Based on your guidelines,
the QoS settings should be something like 750 kbps down, 100 kbps up.


{{< /issue_description >}}

## History
{{< issue_journal date="2011-05-19T20:51:20" author="Dave Täht" >}}
That's very close to what it is set to.

It was my best guess....

Next build.
{{< /issue_journal >}}
{{< issue_journal date="2011-05-20T10:49:16" author="Nick Feamster" >}}
It actually occurred to me that this rate limiting stuff is really going
to mess with our bandwidth tests. We're going to be measuring the QoS
shaper on the router, not the capacity of the access link (unless
there's a way for our tests to evade the shaper... is there?)

Perhaps what we should do is give the user the option to turn it on?
{{< /issue_journal >}}
{{< issue_journal date="2011-05-20T13:23:42" author="Dave Täht" >}}
in your test script:

/etc/init.d/qos stop\
do your test\
/etc/init.d/qos start
{{< /issue_journal >}}
{{< issue_journal date="2011-05-20T13:24:02" author="Dave Täht" >}}
In fact, doing your tests with it on and off is useful...
{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T06:37:09" author="Dave Täht" >}}
It is shocking at just how good connections from south africa -
incredibly long RTT times, but Packet drops are reschedules are minimal.

For some reason ECN appears to be being filtered out along the way as
both endpoints SHOULD be attempting it.

But the crux of this morning's problem is that it appears that the knees
in the curves that should have kicked in RED don't appear to be kicking
in with u/d ratios of 800/400...

On this bug now is a capture, and the values used for the qos-script at
present. (the openwrt qos script can be run with these values on a
router to generate the tc qdisc info which is mildly harder to look at)

Or, the ECN issue could be something like this wiping out the ECN bit?

config 'reclassify'\
option 'target' 'Priority'\
option 'proto' 'tcp'\
option 'pktsize' '-128'\
option 'mark' '!Bulk'\
option 'tcpflags' 'SYN'
{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T06:38:57" author="Dave Täht" >}}
uploaded files... I did! I did...

doing it again
{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T07:13:47" author="Dave Täht" >}}
the "qos overhead" calculation is "off"\
values are set to 840/380

dns is in the "priority" band\
http and httpping is in the "normal" band.\
ping,iperf, and rsync is in the "bulk" band

Various forms of traffic will tweak these in various ways. The qdisc
settings as calculated are:

root@labgw:/usr/lib/qos\# tc qdisc\
qdisc pfifo\_fast 0: dev eth0 root refcnt 2 bands 3 priomap 1 2 2 2 1 2
0 0 1 1 1 1 1 1 1 1\
qdisc hfsc 1: dev eth1 root refcnt 2 default 30\
qdisc sfq 100: dev eth1 parent 1:10 limit 127p quantum 1514b perturb
2sec\
qdisc sfq 200: dev eth1 parent 1:20 limit 127p quantum 1514b perturb
2sec\
qdisc red 300: dev eth1 parent 1:30 limit 29184b min 2432b max 7296b
ecn\
qdisc red 400: dev eth1 parent 1:40 limit 29184b min 2432b max 7296b
ecn\
qdisc ingress ffff: dev eth1 parent ffff:fff1 ----------------\
qdisc mq 0: dev wlan0 root\
qdisc mq 0: dev wlan2 root\
qdisc mq 0: dev mon.wlan0 root\
qdisc mq 0: dev wlan1 root\
qdisc mq 0: dev wlan3 root\
qdisc mq 0: dev wlan5 root\
qdisc mq 0: dev mon.wlan3 root\
qdisc mq 0: dev wlan4 root\
qdisc hfsc 1: dev ifb0 root refcnt 2 default 30\
qdisc sfq 100: dev ifb0 parent 1:10 limit 127p quantum 1514b perturb
2sec\
qdisc sfq 200: dev ifb0 parent 1:20 limit 127p quantum 1514b perturb
2sec\
qdisc red 300: dev ifb0 parent 1:30 limit 63Kb min 5376b max 16128b ecn\
qdisc red 400: dev ifb0 parent 1:40 limit 63Kb min 5376b max 16128b ecn

Running the output of the firewall classification rules here:

One concern of mine is that these rules might be mucking with ecn, but
that does not appear to be the case.

iptables -t mangle -F\
iptables -t mangle -X\
insmod ipt\_multiport &gt;&- 2&gt;&-\
insmod ipt\_CONNMARK &gt;&- 2&gt;&-\
insmod ipt\_length &gt;&- 2&gt;&-\
iptables -t mangle ~~N Default &gt;&~~ 2&gt;&-\
iptables -t mangle ~~N Default\_ct &gt;&~~ 2&gt;&-\
iptables -t mangle -A Default\_ct -m mark --mark 0 -m tcp -p tcp -m
multiport --ports 22,53,222,5060 -j MARK --set-mark 1\
iptables -t mangle -A Default\_ct -m mark --mark 0 -p udp -m udp -m
multiport --ports 22,53,222,5060 -j MARK --set-mark 1\
iptables -t mangle -A Default\_ct -m mark --mark 0 -p tcp -m tcp -m
multiport --ports 20,21,25,80,81,8123,110,443,993,995 -j MARK --set-mark
3\
iptables -t mangle -A Default\_ct -m mark --mark 0 -m tcp -p tcp -m
multiport --ports 5190 -j MARK --set-mark 2\
iptables -t mangle -A Default\_ct -m mark --mark 0 -p udp -m udp -m
multiport --ports 5190 -j MARK --set-mark 2\
iptables -t mangle -A Default\_ct -m mark --mark 0 -m tcp -p tcp -m
multiport --ports 873,5000,5001 -j MARK --set-mark 4\
iptables -t mangle -A Default\_ct -m mark --mark 0 -p udp -m udp -m
multiport --ports 873,5000,5001 -j MARK --set-mark 4\
iptables -t mangle -A Default\_ct -j CONNMARK --save-mark\
iptables -t mangle -A Default -j CONNMARK --restore-mark\
iptables -t mangle -A Default -m mark --mark 0 -j Default\_ct\
iptables -t mangle -A Default -m mark --mark 1 -m length --length 400:
-j MARK --set-mark 0\
iptables -t mangle -A Default -m mark --mark 2 -m length --length 800:
-j MARK --set-mark 0\
iptables -t mangle -A Default -m mark --mark 0 -p udp -m length --length
:500 -j MARK --set-mark 2\
iptables -t mangle -A Default -p icmp -j MARK --set-mark 4\
iptables -t mangle -A Default -m mark --mark 0 -m tcp -p tcp --sport
1024:65535 --dport 1024:65535 -j MARK --set-mark 4\
iptables -t mangle -A Default -m mark --mark 0 -p udp -m udp --sport
1024:65535 --dport 1024:65535 -j MARK --set-mark 4\
iptables -t mangle -A Default -p tcp -m length --length :128 -m mark !
--mark 4 -m tcp --tcp-flags ALL SYN -j MARK --set-mark 1\
iptables -t mangle -A Default -p tcp -m length --length :128 -m mark !
--mark 4 -m tcp --tcp-flags ALL ACK -j MARK --set-mark 1\
iptables -t mangle -A OUTPUT -o eth1 -j Default\
iptables -t mangle -A FORWARD -o eth1 -j Default
{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T09:00:10" author="Dave Täht" >}}
The knee in the curve of this calculation appears to be problematic
(from /usr/lib/qos

                   printf "tc qdisc add dev "device" parent 1:"class[i]"0 handle "class[i]"00: "

                    # RED parameters - also used to determine the queue length for sfq
                    # calculate min value. for links <= 256 kbit, we use 1500 bytes
                    # use 50 ms queue length as min threshold for faster links
                    # max threshold is fixed to 3*min
                    base_pkt=3000
                    base_rate=256
                    min_lat=50
                    if (maxrate[i] <= base_rate) min = base_pkt
                    else min = int(maxrate[i] * 1024 / 8 * 0.05)
                    max = 3 * min
                    limit = (min + max) * 3

                    if (qdisc[i] != "") {
                            # user specified qdisc
                            print qdisc[i] " limit " limit
                    } else if (rtm1[i] > 0) {
                            # rt class - use sfq
                            print "sfq perturb 2 limit "  limit
                    } else {
                            # non-rt class - use RED

                            avpkt = pktsize[i]
                            # don't use avpkt values less than 500 bytes
                            if (avpkt < 500) avpkt = 500
                            # if avpkt is too close to min, scale down avpkt to allow proper bursting
                            if (avpkt > min * 0.70) avpkt *= 0.70


                            # according to http://www.cs.unc.edu/~jeffay/papers/IEEE-ToN-01.pdf a drop
                            # probability somewhere between 0.1 and 0.2 should be a good tradeoff
                            # between link utilization and response time (0.1: response; 0.2: utilization)
                            prob="0.12"
                            rburst=int((2*min + max) / (3 * avpkt))
                            if (rburst < 2) rburst = 2
                            print "red min " min " max " max " burst " rburst " avpkt " avpkt " limit " limit " probability " prob " ecn"
                    }
            }

{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T12:47:39" author="Dave Täht" >}}
Some dropped by both red and hfsc, very few marked... need to look at
the traces.

root@labgw:\~\# tc -s qdisc\
qdisc pfifo\_fast 0: dev eth0 root refcnt 2 bands 3 priomap 1 2 2 2 1 2
0 0 1 1 1 1 1 1 1 1\
Sent 1614863066 bytes 1327262 pkt (dropped 3254, overlimits 0 requeues
12093)\
backlog 0b 0p requeues 12093\
qdisc hfsc 1: dev eth1 root refcnt 2 default 30\
Sent 85724166 bytes 88100 pkt (dropped 1155, overlimits 119507 requeues
0)\
backlog 0b 0p requeues 0\
qdisc sfq 100: dev eth1 parent 1:10 limit 127p quantum 1514b perturb
2sec\
Sent 297729 bytes 3345 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc sfq 200: dev eth1 parent 1:20 limit 127p quantum 1514b perturb
2sec\
Sent 171159 bytes 2209 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc red 300: dev eth1 parent 1:30 limit 29184b min 2432b max 7296b
ecn\
Sent 85134125 bytes 81686 pkt (dropped 1154, overlimits 1155 requeues
0)\
backlog 0b 0p requeues 0\
marked 1 early 1154 pdrop 0 other 0\
qdisc red 400: dev eth1 parent 1:40 limit 29184b min 2432b max 7296b
ecn\
Sent 121153 bytes 860 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
marked 0 early 0 pdrop 0 other 0\
qdisc ingress ffff: dev eth1 parent ffff:fff1 ----------------\
Sent 43661931 bytes 211410 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc mq 0: dev wlan0 root\
Sent 14518331 bytes 68606 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc mq 0: dev wlan2 root\
Sent 7895632 bytes 39007 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc mq 0: dev mon.wlan0 root\
Sent 7188274 bytes 43046 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc mq 0: dev wlan1 root\
Sent 10673024 bytes 45544 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc mq 0: dev wlan3 root\
Sent 14518199 bytes 68604 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc mq 0: dev wlan5 root\
Sent 8190424 bytes 46371 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc mq 0: dev mon.wlan3 root\
Sent 3939780 bytes 24626 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc mq 0: dev wlan4 root\
Sent 10662036 bytes 45578 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc hfsc 1: dev ifb0 root refcnt 2 default 30\
Sent 40132206 bytes 153407 pkt (dropped 496, overlimits 63534 requeues
0)\
backlog 0b 0p requeues 0\
qdisc sfq 100: dev ifb0 parent 1:10 limit 127p quantum 1514b perturb
2sec\
Sent 305151 bytes 2660 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc sfq 200: dev ifb0 parent 1:20 limit 127p quantum 1514b perturb
2sec\
Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
qdisc red 300: dev ifb0 parent 1:30 limit 63Kb min 5376b max 16128b ecn\
Sent 39806046 bytes 150717 pkt (dropped 496, overlimits 496 requeues 0)\
backlog 0b 0p requeues 0\
marked 0 early 496 pdrop 0 other 0\
qdisc red 400: dev ifb0 parent 1:40 limit 63Kb min 5376b max 16128b ecn\
Sent 21009 bytes 30 pkt (dropped 0, overlimits 0 requeues 0)\
backlog 0b 0p requeues 0\
marked 0 early 0 pdrop 0 other 0
{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T15:32:39" author="David Taht" >}}
Here is some real data on a typical use pattern,\
which is simply using google with it's "interactive" feature turned on
and\
adblock plus enabled on my browser, while duplicating your QoS
settings.\
(Without adblock, it is much worse)

http://gw.lab.bufferbloat.net/captures/series1/qos1googleabcde-withecn.cap

View the data in wireshark.

Note the number of DNS queries. Note the timestamps of the DNS queries.
Note\
packet lossage and retransmits.

To repeat this particular test, login to google, go to their search
page, go\
to the settings thing on the top right of the window, turn on the\
"interactive" feature...

login to the router and

/etc/init.d/dnsmasq stop\
/etc/init.d/dnsmasq start\
tcpdump -i eth1 -w/tmp/whatever.cap

search for a \# wait a few seconds for it all to load\
backspace, then b\
backspace, then c\
backspace, then d\
backspace, then e

hit control-C on the router to halt the dump (it should be less than
200k),\
then transfer the file somewhere\
and take a look at it.

Repeat with QoS on and off.

I would appreciate a copy, too.

On Sat, May 21, 2011 at 3:47 PM, Nick Feamster
<feamster@cc.gatech.edu>wrote:

> I'm up, Babel is not. :-) That configuration wiped me out, so I
decided to\
> try the new build.

I would have liked it had you stuck with it long enough to talk to me.
Did\
you take a backup?

> Clifton is back online. I'm bringing seapoint up shortly, then
heading to\
> bed.\
>

K. Same password?

> The chrome stuff looks nice on the router now! I think the
"configuration\
> page" could feature a bit more prominently. I was going to boldface
that\
> line and check that in.\
>

Go right ahead. I am not doing any more builds for a while, unless
something\
major happens. I can certainly disable qos by default in the next build.

Please also check "package updates" they should come from the right
place\
now, although the xwrt repo was still not up to date when last I looked.

>\
> Srikanth and I chatted about the QoS stuff at dinner. Given the
widespread\
> disagreement on this, I think we have a research paper on our
hands. :-)

Well, yes.

> My two cents: disabling this will make user experience no worse
than what\
> they have now.

I only have data from about a dozen cybercafes in Nicaragua, and
several\
thousand devices in california, 6 months of intensive research into the\
bufferbloat issue, 100s of thousands of educated users doing something
like\
this to their routers (and 100s of millions, not), and several hundred\
papers to back me up.

> Enabling it could make the experience better, but it could also
make it\
> perceivably worse if we get it wrong.

Getting it wrong consists of entering too high or too low values. Too
high,\
and it does little good (although packet prioritization also in there
helps\
regardless), too low, and long lived tcp streams suffer.

Not setting it will make it perceivably worse in many respects.

The interesting thing to me, thus far, was finding a knee in the curve
under\
your conditions that resulted in 30+% loss of peak performance, rather
than\
a more reasonable, 12%. The results I get here are decidedly different,\
using the same test conditions, I'm currently getting throttled
throughput\
in excess of 10% HIGHER than what is the setting. At the moment I'm
looking\
into the mars polar lander-like problem of the difference between
megabits\
and mibbibits, but that would account for less than 5% of the
discrepancy.

Your long RTT times change how QoS functions by quite a bit.

My thought would be:

1\) get the boxes deployed.\
2) Make sure people are happy.\
2a) do a little testing, see below\
3) Then, we can communicate with them to make sure they know that they
have\
an option to turn this on.\
4) Or, we turn it on, but we tell them that we're doing so, and only do
it\
after they aren't complaining about the box for other reasons.

>\
> Multi-user testing under realistic conditions is helpful.

A\) Make a skype phone call while doing a download.\
B) Do an upload while downloading a large file\
C) run bittorrent in the background and try to do anything.

I'll rely on feed back from testers such as vincent and jg for my data
until\
we can clearly distinguish between sites, or you get more data by going
into\
the field and dealing with real users and real usage on the deployed\
equipment..

-Nick\
>\
>
{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T15:40:05" author="Dave Täht" >}}
I was salso delighted to get the feedback from the user in canada:

<irving> Connecting to Bell Canada using PPPOE, speedtest.net shows\
me 6MPBS down, 420K up using my old d-dlink DIR615

<irving> turned QOS off, got 6M up 420K down (daisy chained through\
my other router, didn't get around to swapping it out of the\
path)\
<irving> using bismark, I get 870K down 70K up. This is on\
WNDR3700v2, squashfs-factory-NA.img\
<dtaht> that's about right ish - it's set for 1000/100 I think.\
<dtaht> If you change QoS to match what you measure,\
<dtaht> you should get very close to that speed.

<irving> yup, looks much nicer. Still through the chain, QOS set to\
420K up 5800 K down, speedtest.net shows 5.15M up 320 down\
which is pretty close

I so love having random testing from random strangers...

<irving> I'm a random stranger who stumbled over your project via\
gettys postings about bufferbloat. used to work on\
BorderWare and Secure Computing firewalls so I have some\
(very out of date) chops.
{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T15:43:26" author="Srikanth Sundaresan" >}}
Can't test. :(\
Damn connection sharing doesn't work on my mac, can't get my router
online.

On May 22, 2011, at 12:32 AM, Dave Taht wrote:

>\
> Here is some real data on a typical use pattern,\
> which is simply using google with it's "interactive" feature turned
on and adblock plus enabled on my browser, while duplicating your QoS
settings. (Without adblock, it is much worse)\
>\
>
http://gw.lab.bufferbloat.net/captures/series1/qos1googleabcde-withecn.cap
>\
> View the data in wireshark.\
>\
> Note the number of DNS queries. Note the timestamps of the DNS
queries. Note packet lossage and retransmits.\
>\
> To repeat this particular test, login to google, go to their search
page, go to the settings thing on the top right of the window, turn on
the "interactive" feature...\
>\
> login to the router and\
>\
> /etc/init.d/dnsmasq stop\
> /etc/init.d/dnsmasq start\
> tcpdump -i eth1 -w/tmp/whatever.cap\
>\
> search for a \# wait a few seconds for it all to load\
> backspace, then b\
> backspace, then c\
> backspace, then d\
> backspace, then e\
>\
> hit control-C on the router to halt the dump (it should be less
than 200k), then transfer the file somewhere\
> and take a look at it.\
>\
> Repeat with QoS on and off.\
>\
> I would appreciate a copy, too.\
>\
>\
> On Sat, May 21, 2011 at 3:47 PM, Nick Feamster
<feamster@cc.gatech.edu> wrote:\
> I'm up, Babel is not. :-) That configuration wiped me out, so I
decided to try the new build.\
>\
> I would have liked it had you stuck with it long enough to talk to
me. Did you take a backup?\
>\
> Clifton is back online. I'm bringing seapoint up shortly, then
heading to bed.\
>\
> K. Same password?\
>\
> The chrome stuff looks nice on the router now! I think the
"configuration page" could feature a bit more prominently. I was going
to boldface that line and check that in.\
>\
> Go right ahead. I am not doing any more builds for a while, unless
something major happens. I can certainly disable qos by default in the
next build.\
>\
> Please also check "package updates" they should come from the right
place now, although the xwrt repo was still not up to date when last I
looked.\
>\
>\
> Srikanth and I chatted about the QoS stuff at dinner. Given the
widespread disagreement on this, I think we have a research paper on our
hands. :-)\
>\
> Well, yes.\
>\
> My two cents: disabling this will make user experience no worse
than what they have now.\
>\
>\
> I only have data from about a dozen cybercafes in Nicaragua, and
several thousand devices in california, 6 months of intensive research
into the bufferbloat issue, 100s of thousands of educated users doing
something like this to their routers (and 100s of millions, not), and
several hundred papers to back me up.\
>\
>\
> Enabling it could make the experience better, but it could also
make it perceivably worse if we get it wrong.\
>\
> Getting it wrong consists of entering too high or too low values.
Too high, and it does little good (although packet prioritization also
in there helps regardless), too low, and long lived tcp streams suffer.\
>\
> Not setting it will make it perceivably worse in many respects.\
>\
> The interesting thing to me, thus far, was finding a knee in the
curve under your conditions that resulted in 30+% loss of peak
performance, rather than a more reasonable, 12%. The results I get here
are decidedly different, using the same test conditions, I'm currently
getting throttled throughput in excess of 10% HIGHER than what is the
setting. At the moment I'm looking into the mars polar lander-like
problem of the difference between megabits and mibbibits, but that would
account for less than 5% of the discrepancy.\
>\
> Your long RTT times change how QoS functions by quite a bit.\
>\
> My thought would be:\
>\
> 1) get the boxes deployed.\
> 2) Make sure people are happy.\
> 2a) do a little testing, see below\
> 3) Then, we can communicate with them to make sure they know that
they have an option to turn this on.\
> 4) Or, we turn it on, but we tell them that we're doing so, and
only do it after they aren't complaining about the box for other
reasons.\
>\
> Multi-user testing under realistic conditions is helpful.\
>\
> A) Make a skype phone call while doing a download.\
> B) Do an upload while downloading a large file\
> C) run bittorrent in the background and try to do anything.\
>\
> I'll rely on feed back from testers such as vincent and jg for my
data until we can clearly distinguish between sites, or you get more
data by going into the field and dealing with real users and real usage
on the deployed equipment..\
>\
> -Nick\
>\
>
{{< /issue_journal >}}
{{< issue_journal date="2011-05-21T15:48:55" author="Dave Täht" >}}
the numbers in the lab (using wget on a large file on huchra which runs
for 2+ minutes) show 97KB/sec when set to 840 dn/ 420 up. I suspect part
of this is due to a mibbi vs megabit discrepancy in the reporting.

The numbers reported from a speedtest user in canada show roughly a 25%
non-relationship to his actual speed ratio of 6M down, 420 up. I don't
know what he was using to perform his actual tests. The ratio he has
there is perilously close to the maximum possible for normal TCP/ip acks
to get back (21x1 is about the best you can do)

Not knowing the exact data for the current defaults in use in South
Africa, it soundes as though the performance difference there for a
speedtest is over 30%.
{{< /issue_journal >}}
{{< issue_journal date="2011-05-22T05:58:02" author="Dave Täht" >}}
I went and looked at the performance this morning from capetown, for
uploads.

For reference, an unloaded ping:

64 bytes from 149.20.54.82: seq=12 ttl=46 time=310.525 ms

Without QoS set, and doing a scp upload, while pinging. Random samples.

noqos.cap 71% 2212KB 41.2KB/s 00:21 from 149.20.54.82: seq=72 ttl=46
time=454.703 ms\
noqos.cap 73% 2260KB 41.9KB/s 00:19 from 149.20.54.82: seq=73 ttl=46
time=604.203 ms\
noqos.cap 74% 2308KB 42.5KB/s 00:18 from 149.20.54.82: seq=74 ttl=46
time=599.380 ms\
noqos.cap 76% 2348KB 42.3KB/s 00:17 from 149.20.54.82: seq=75 ttl=46
time=331.745 ms\
noqos.cap 77% 2380KB 41.2KB/s 00:17 from 149.20.54.82: seq=76 ttl=46
time=474.808 ms

With QoS set to 840/380 and ping in the "Bulk" band.

noqos.cap 53% 1660KB 32.5KB/s from 149.20.54.82: seq=137 ttl=46
time=327.042 ms\
noqos.cap 55% 1700KB 33.2KB/s from 149.20.54.82: seq=138 ttl=46
time=310.355 ms\
noqos.cap 56% 1732KB 33.1KB/s from 149.20.54.82: seq=139 ttl=46
time=332.353 ms\
noqos.cap 57% 1780KB 34.6KB/s from 149.20.54.82: seq=140 ttl=46
time=331.784 ms

With QoS set to 1000/500 you can see the "bulk" band start to kick in,
and\
ping jittering harder

noqos.cap 67% 2092KB 35.6KB/s bytes from 149.20.54.82: seq=84 ttl=46
time=352.532 ms\
noqos.cap 69% 2132KB 36.1KB/s bytes from 149.20.54.82: seq=85 ttl=46
time=354.672 ms\
noqos.cap 70% 2164KB 35.7KB/s bytes from 149.20.54.82: seq=86 ttl=46
time=310.943 ms\
noqos.cap 71% 2212KB 36.9KB/s bytes from 149.20.54.82: seq=87 ttl=46
time=322.205 ms\
noqos.cap 72% 2236KB 35.6KB/s bytes from 149.20.54.82: seq=88 ttl=46
time=366.960 ms\
noqos.cap 73% 2276KB 36.0KB/s bytes from 149.20.54.82: seq=89 ttl=46
time=349.261 ms\
noqos.cap 74% 2308KB 35.6KB/s bytes from 149.20.54.82: seq=90 ttl=46
time=341.993 ms

I will definitely argue that second derived QoS calculation has a knee
in it that needs\
to be tweaked upon further analysis of the packet traces,\
but for more multi-threaded usage (such as a typical web browser, or\
browsers, or multiple users, and making phone calls), the default QoS is
doing it's job fairly well\
at the **lowest** settings. Going more than 20% above will probably
affect voice traffic.
{{< /issue_journal >}}
{{< issue_journal date="2011-05-22T06:35:01" author="Dave Täht" >}}
QoS set to levels reported by speedtest seem to be reporting "safe"
values. Going 20% higher seems feasible on my limited testing here, but
I would argue for measuring twice and cutting once under realistic
multiuser conditions.

What decisions you make from here regarding deployment are not up to me.
It would be good if you gathered data on the settings of QoS in the
field, off/on/set to what/using what.
{{< /issue_journal >}}
{{< issue_journal date="2011-05-23T03:38:40" author="Nick Feamster" >}}
Your experiment above is really interesting. I think it makes sense to
do testing with/without QoS, for various settings.

We will have to let people know that we're doing this, of course.
{{< /issue_journal >}}
{{< issue_journal date="2011-05-23T03:56:19" author="Nick Feamster" >}}

{{< /issue_journal >}}
{{< issue_journal date="2011-05-23T03:56:50" author="Nick Feamster" >}}

{{< /issue_journal >}}
{{< issue_journal date="2011-05-23T18:45:42" author="Irving Reid" >}}
Dave Täht wrote:\
> the numbers in the lab (using wget on a large file on huchra which
runs for 2+ minutes) show 97KB/sec when set to 840 dn/ 420 up. I suspect
part of this is due to a mibbi vs megabit discrepancy in the reporting.\
>\
> The numbers reported from a speedtest user in canada show roughly a
25% non-relationship to his actual speed ratio of 6M down, 420 up. I
don't know what he was using to perform his actual tests. The ratio he
has there is perilously close to the maximum possible for normal TCP/ip
acks to get back (21x1 is about the best you can do)\
>\
> Not knowing the exact data for the current defaults in use in South
Africa, it soundes as though the performance difference there for a
speedtest is over 30%.

FYI, it was a Mac (OS X 6.7), running a basic test using the
"www.speedtest.net" web application under Safari, testing against nearby
servers (as chosen by speedtest.net); I also had a ping running in the
background. I was getting tons of lost ping packets until I adjusted my
QOS settings to move ICMP into the "express" packet class. I have my QOS
bandwidth set to 5800 up, 400 down.

With that, ping huchra.bufferbloat.net is giving me a pretty stable
82-83 ms RTT right now; when I start a speedtest, the RTT to huchra goes
up to about 150ms during the download speed portion, and to about 100ms
during the upload test. speedtest.net reports 5.39 Mbps up, 0.37Mbps
down.
{{< /issue_journal >}}
{{< issue_journal date="2011-05-27T18:16:54" author="Dave Täht" >}}
@Irving

THANK YOU For the data. You'd also mentioned you were using bittorrent
and the QoS was successfully helping in that regard?

I still find a factor of two difference for so-called "express" traffic
to be unacceptable.

I'd also like to find a better way to classify ping. Making it be
express is good for testing, but making it be "bulk" lets you see qos
starting to kick in. Perhaps classifying different size pings
differently would let the various bands be more visible.

And packet loss is a GOOD thing.
{{< /issue_journal >}}
{{< issue_journal date="2011-05-27T19:25:41" author="Dave Täht" >}}
@irving: Also, for reference, what does speedtest report for qos off,
while pinging?

Secondly, do you have the QoS overhead adjustment on or off? For ppoe it
should probably be on, but I'd have to talk to the adsl-optimizer guy to
truly understand ppoe connections.
{{< /issue_journal >}}

